# -*- coding: utf-8 -*-
"""Indian_Constitution_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LryKlej52B8y-dXZ7reuimaGftsk457y
"""

pip install llama-index google-generativeai pinecone-client

pip install llama-index

pip install google-generativeai

pip install pinecone-client

pip install llama-index-llms-gemini
pip install llama-index-embeddings-gemini
pip install llama-index-vector-stores-pinecone

!pip install llama-index-llms-gemini llama-index-embeddings-gemini llama-index-vector-stores-pinecone

import os
from pinecone import Pinecone
from llama_index.llms.gemini import Gemini
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core import StorageContext, VectorStoreIndex, download_loader
from llama_index.core import Settings

def set_api_keys(google_key, pinecone_key):
    os.environ["GOOGLE_API_KEY"] = google_key
    os.environ["PINECONE_API_KEY"] = pinecone_key
    print("API keys set successfully!")


def initialize_models():
    llm = Gemini()
    embed_model = GeminiEmbedding(model_name="models/embedding-001")
    return llm, embed_model


def configure_settings(llm, embed_model, chunk_size=1024):
    Settings.llm = llm
    Settings.embed_model = embed_model
    Settings.chunk_size = chunk_size
    print(f"Settings configured: LLM={llm}, Embed Model={embed_model}, Chunk Size={chunk_size}")


if __name__ == "__main__":

    GOOGLE_API_KEY = "AIzaSyBPGm7SdA8KfNv6wCrecttkQbPvM6V0ZiE"
    PINECONE_API_KEY = "31e1ca39-55b6-4698-9d2b-95c37511e235"

    set_api_keys(GOOGLE_API_KEY, PINECONE_API_KEY)

    llm, embed_model = initialize_models()

    configure_settings(llm, embed_model)

def initialize_pinecone_client():
    api_key = os.getenv("PINECONE_API_KEY")
    if api_key:
        pinecone_client = Pinecone(api_key=api_key)
        print("Pinecone client initialized successfully!")
        return pinecone_client
    else:
        raise EnvironmentError("Pinecone API key not found in environment variables.")

pinecone_client = initialize_pinecone_client()

def list_pinecone_indexes(pinecone_client):
    indexes = pinecone_client.list_indexes()
    if indexes:
        print("Available indexes in Pinecone:")
        for index in indexes:
            print(f"- {index['name']}")
    else:
        print("No indexes found in Pinecone.")

list_pinecone_indexes(pinecone_client)

def describe_pinecone_index(pinecone_client, index_name):
    print(f"\nAttempting to describe index: '{index_name}'")

    try:
        # Retrieve index description
        index_description = pinecone_client.describe_index(index_name)

        # Extract and print key information from the description
        print("\n--- Index Description ---")
        print(f"Index Name     : {index_description['name']}")
        print(f"Dimension      : {index_description['dimension']}")
        print(f"Metric Type    : {index_description['metric_type']}")
        print(f"Number of Shards: {index_description['num_shards']}")
        print(f"Status         : {index_description.get('status', 'Not available')}")
        print(f"Metadata       : {index_description.get('metadata', 'No metadata available')}")
        print("--- End of Description ---\n")

    except Exception as e:
        print(f"Failed to describe index '{index_name}'. Error: {e}\n")

describe_pinecone_index(pinecone_client, "knowledgeagent")

index_description= pinecone_client.describe_index("knowledgeagent")
print(index_description)

import os
print(os.getcwd())

os.makedirs("data", exist_ok=True)

from google.colab import files
uploaded = files.upload()

os.rename("/content/constitution.pdf", "data/constitution.pdf")

from llama_index.core import SimpleDirectoryReader
documents = SimpleDirectoryReader("data").load_data()

documents

# Initialize Pinecone index
pinecone_index = pinecone_client.Index("knowledgeagent")

def create_vector_store_and_context(pinecone_index):

    vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
    print("PineconeVectorStore created successfully.")

    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    print("StorageContext created with the vector store.")

    return vector_store, storage_context

def create_index_from_documents(documents, storage_context):
    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
    print("Index created from documents.")
    return index

vector_store, storage_context = create_vector_store_and_context(pinecone_index)
index = create_index_from_documents(documents, storage_context)

chat_engine = index.as_chat_engine()

def run_chatbot(chat_engine):
    print("Chatbot is running. Type 'exit' to quit.\n")

    while True:
        text_input = input("User: ")

        if text_input.lower() == "exit":
            print("Exiting chatbot. Goodbye!")
            break
        try:
            response = chat_engine.chat(text_input)
            print(f"Agent: {response.response}\n")

        except Exception as e:
            print(f"An error occurred during the query: {str(e)}\n")

run_chatbot(chat_engine)

pip install streamlit
